# =============================================================================
# TRANSCRIPTION LIBRARY CONFIGURATION
# =============================================================================
# Copy this file to .env and configure the values according to your needs

# =============================================================================
# GENERAL CONFIGURATION
# =============================================================================

# Primary transcription provider to use by default
# Options: distil-whisper-pt, faster-whisper, gemini-hybrid
PRIMARY_PROVIDER=distil-whisper-pt

# Fallback providers in order (comma-separated)
# If the primary provider fails, these will be tried in order
FALLBACK_PROVIDERS=faster-whisper,gemini-hybrid

# Minimum confidence threshold to accept transcription results (0.0-1.0)
# Lower values accept more results but may include lower quality transcriptions
CONFIDENCE_THRESHOLD=0.7

# =============================================================================
# FASTER-WHISPER CONFIGURATION
# =============================================================================

# Model size for Faster-Whisper transcription
# Options: tiny, base, small, medium, large-v2, large-v3
# Larger models are more accurate but require more resources
FASTER_WHISPER_MODEL_SIZE=medium

# Force CPU usage for Faster-Whisper even if GPU is available
# Set to true if you experience GPU-related issues
FASTER_WHISPER_FORCE_CPU=false

# =============================================================================
# DISTIL-WHISPER CONFIGURATION (PT-BR OPTIMIZED)
# =============================================================================

# Cache directory for downloaded Distil-Whisper models
# Models will be stored here to avoid re-downloading
DISTIL_WHISPER_CACHE_DIR=./models/cache

# Force CPU usage for Distil-Whisper even if GPU is available
# Set to true if you experience GPU-related issues
FORCE_CPU_FOR_DISTIL_WHISPER=false

# =============================================================================
# GEMINI VIDEO CONFIGURATION
# =============================================================================

# Your Google AI Studio API key for Gemini services
# Get one at: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Maximum video file size in MB for Gemini processing
# Files larger than this will be rejected
MAX_VIDEO_SIZE_MB=1000

# Timeout for video processing operations in seconds
# Increase for longer videos or slower connections
GEMINI_VIDEO_TIMEOUT=300

# =============================================================================
# SYSTEM CONFIGURATION (OPTIONAL)
# =============================================================================

# Path to ffprobe binary (leave empty for auto-detection)
# Only set if ffprobe is not in your system PATH
FFPROBE_PATH=/usr/bin/ffprobe

# Path to CUDA libraries for GPU acceleration
# Only needed if CUDA is installed in a non-standard location
CUDNN_PATH=C:\Program Files\NVIDIA\CUDNN\v9.10